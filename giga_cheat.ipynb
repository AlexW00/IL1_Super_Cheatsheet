{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPER CHEAT NOTEBOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for the GigaCheat tool\n",
    "def print_seperator():\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, log_loss\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4).pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(_z):\n",
    "    return 1 / (1 + np.exp(-_z))\n",
    "\n",
    "def z(_x, _w, _b):\n",
    "    return np.dot(_x, _w) + _b\n",
    "\n",
    "def cross_entropy_loss (predicted_value, actual_value):\n",
    "    return -actual_value * math.log(predicted_value) - (1 - actual_value) * np.log(1 - predicted_value)\n",
    "\n",
    "def cross_entropy_loss_vector (_y_true_vector, _y_pred_vector):\n",
    "    log_loss(_y_true_vector, _y_pred_vector, labels=[1,0])\n",
    "\n",
    "\n",
    "def decision_function(val, _decision_boundary=0.5):\n",
    "    return 1 if val >= _decision_boundary else 0\n",
    "\n",
    "def calc_y_pred(_y_pred_prob, _decision_boundary=0.5):\n",
    "    _y_pred = []\n",
    "    for val in _y_pred_prob:\n",
    "        _y_pred.append(decision_function(val))\n",
    "    return _y_pred\n",
    "\n",
    "def calc_pred_prob(_y_true, _X, _w, _b):\n",
    "    _y_pred_prob = []\n",
    "    for i in range(len(_X)):\n",
    "        _y_pred_prob.append(sigmoid(z(_X[i], _w, _b)))\n",
    "    return _y_pred_prob\n",
    "\n",
    "def gradient_descent(_X, _y, _w, _b, _learning_rate=0.01, _num_iterations=100):\n",
    "    for i in range(0, _num_iterations):\n",
    "        _w_gradient = np.zeros(len(_w))\n",
    "        _b_gradient = 0\n",
    "        for j in range(len(_X)):\n",
    "            _x = _X[j]\n",
    "            _z = z(_x, _w, _b)\n",
    "            for k in range(len(_w)):\n",
    "                _w_gradient[k] = (_z - _y[j]) * _x[k]\n",
    "            _b_gradient = _z - _y[j]\n",
    "\n",
    "            _w = _w - _learning_rate * _w_gradient\n",
    "            _b = _b - _learning_rate * _b_gradient  \n",
    "    return _w, _b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels = [0, 1] \n",
      "y_true: [1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "X = [[0.6, 0.9], [0.1, 0.2], [0.5, 0.9], [0.8, 0.5], [0.2, 0.1], [0.4, 0.2], [0.6, 0.9], [0.1, 0.2], [0.5, 0.9], [0.8, 0.5], [0.2, 0.1], [0.4, 0.2]] \n",
      "w = [0.9612927  0.94306953] \n",
      "b = -0.35715637887815543 \n",
      "decision_boundary 0.5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "y_pred_prob = [0.7442890600767808, 0.4819046056457873, 0.725568250154638, 0.7075288845602947, 0.48235960306812675, 0.5537847479248225, 0.7442890600767808, 0.4819046056457873, 0.725568250154638, 0.7075288845602947, 0.48235960306812675, 0.5537847479248225] \n",
      "y_pred = [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "labels = [0,1]\n",
    "y_true = [1,0,1, 1, 0, 0, 1,0,1, 1, 0, 0] # the actual labels\n",
    "\n",
    "y_pred_prob = [] # = sigmoid of z-score of input vector x of X\n",
    "y_pred = [] # = 1 if y_pred_prob >= decision_boundary, 0 otherwise\n",
    "X = [[0.6, 0.9], [0.1, 0.2], [0.5, 0.9], [0.8, 0.5], [0.2, 0.1], [0.4, 0.2], [0.6, 0.9], [0.1, 0.2], [0.5, 0.9], [0.8, 0.5], [0.2, 0.1], [0.4, 0.2]] # observed input vectors\n",
    "training_iterations = 10000 # number of iterations for gradient descent\n",
    "w, b = gradient_descent(X, y_true, [.5,.5], 1, 0.01, training_iterations) # w and b are the weights and bias of the model\n",
    "\n",
    "decision_boundary = 0.5 # decision boundary for the model\n",
    "\n",
    "y_pred_prob = calc_pred_prob(y_true, X, w, b)\n",
    "y_pred = calc_y_pred(y_pred_prob, decision_boundary)\n",
    "\n",
    "\n",
    "print(\"labels =\", labels, \"\\ny_true:\", y_true)\n",
    "print_seperator()\n",
    "print(\"X =\", X, \"\\nw =\", w, \"\\nb =\", b, \"\\ndecision_boundary\", decision_boundary)\n",
    "print_seperator()\n",
    "print(\"y_pred_prob =\", y_pred_prob, \"\\ny_pred =\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_true: [1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0] \n",
      "y_pred: [1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "        Pred 0  Pred 1\n",
      "True 0       4       2\n",
      "True 1       0       6\n",
      "True positives: 1 True negatives: 3 False positives: 0 False negatives: 2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Precision: 0.75 Recall: 1.0 F1-Balanced: 0.8571428571428571 Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# y_true = [1, 1, 0]\n",
    "# y_pred = [1, 1, 1]\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred):\n",
    "    confusion_matrix_micro = confusion_matrix(y_true, y_pred)\n",
    "    label_texts_index = ['True 0', 'True 1']\n",
    "    label_texts_value = ['Pred 0', 'Pred 1']\n",
    "    cmtx = pd.DataFrame(\n",
    "        confusion_matrix(y_true, y_pred), \n",
    "        label_texts_index, \n",
    "        label_texts_value\n",
    "    )\n",
    "    print(cmtx)\n",
    "\n",
    "def print_tp_fp_tn_fn(y_true, y_pred):\n",
    "    tp, fp, fn, tn = confusion_matrix_micro.ravel()\n",
    "    print(\"True positives:\", tp, \"True negatives:\", tn, \"False positives:\", fp, \"False negatives:\", fn)\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print_seperator()\n",
    "    precision, recall, f1, accuracy = precision_score(y_true, y_pred), recall_score(y_true, y_pred), f1_score(y_true, y_pred), accuracy_score(y_true, y_pred)\n",
    "    print(\"Precision:\", precision, \"Recall:\", recall, \"F1-Balanced:\", f1, \"Accuracy:\", accuracy)\n",
    "print(\"y_true:\", y_true, \"\\ny_pred:\", y_pred)\n",
    "print_seperator()\n",
    "print_confusion_matrix(y_true, y_pred)\n",
    "print_tp_fp_tn_fn(y_true, y_pred)\n",
    "print_metrics(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi (_w_count, _c_count, _wc_count, _total):\n",
    "    p_w = _w_count / _total\n",
    "    p_c = _c_count / _total\n",
    "    p_wc = _wc_count / _total\n",
    "    return math.log2(p_wc / (p_w * p_c))\n",
    "\n",
    "def ppmi (pmi): \n",
    "    if (pmi < 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return pmi\n",
    "\n",
    "def tF (_num_t_in_doc):\n",
    "    if (_num_t_in_doc < 0):\n",
    "        return 0\n",
    "    else: \n",
    "        return math.log10(1 + _num_t_in_doc)\n",
    "\n",
    "def idF (_num_doc_with_t, _total_docs):\n",
    "    return math.log(_total_docs/_num_doc_with_t)\n",
    "\n",
    "def tFidF(_tF, _idF):\n",
    "    return _tF * _idF\n",
    "\n",
    "def centroid_document_vector(d):\n",
    "    return np.mean(d, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> PMI, PPMI\n",
      "x = [0, 0, 5, 1] \n",
      "y = [0, 1, 0, 1]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "w_count = 3 \n",
      "c_count = 4 \n",
      "wc_count = 2 \n",
      "total = 19\n",
      "Mutual information score: 0.3465735902799726\n",
      "PMI: 1.6629650127224294 PPMI: 1.6629650127224294\n",
      "----------------------------------------------------------------------------------------------------\n",
      "> TF-IDF\n",
      "num_t_in_doc = 3 \n",
      "num_doc_with_t = 2 \n",
      "total_docs = 5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tF: 0.6020599913279624 idF: 0.9162907318741551 tFidF: 0.5516619900860461\n"
     ]
    }
   ],
   "source": [
    "#pmi: compare two words by their co-occurence\n",
    "x = [0,0,5,1]\n",
    "y = [0,1,0,1]\n",
    "\n",
    "w_count = 3 # count of word w\n",
    "c_count = 4 # count of words in context c\n",
    "wc_count = 2 # count of word w in context c\n",
    "total = 19 # total number of words in corpus\n",
    "\n",
    "# tf-idf: relevancy score of a word w in a document d\n",
    "num_t_in_doc = 3 # number of times word w appears in document d\n",
    "num_doc_with_t = 2 # number of documents in corpus with word w\n",
    "total_docs = 5 # total number of documents in corpus\n",
    "\n",
    "\n",
    "print(\"> PMI, PPMI\")\n",
    "print(\"x =\", x, \"\\ny =\", y)\n",
    "print_seperator()\n",
    "print(\"w_count =\", w_count, \"\\nc_count =\", c_count, \"\\nwc_count =\", wc_count, \"\\ntotal =\", total)\n",
    "print(\"Mutual information score:\", mutual_info_score(x, y))\n",
    "print(\"PMI:\", pmi(w_count, c_count, wc_count, total), \"PPMI:\", ppmi(pmi(w_count, c_count, wc_count, total)))\n",
    "print_seperator()\n",
    "print(\"> TF-IDF\")\n",
    "print(\"num_t_in_doc =\", num_t_in_doc, \"\\nnum_doc_with_t =\", num_doc_with_t, \"\\ntotal_docs =\", total_docs)\n",
    "print_seperator()\n",
    "print(\"tF:\", tF(num_t_in_doc), \"idF:\", idF(_num_doc_with_t, _total_docs), \"tFidF:\", tFidF(tF(num_t_in_doc), idF(num_doc_with_t, total_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_length(v):\n",
    "    return np.sqrt(np.dot(v, v))\n",
    "\n",
    "def dot_product(v1, v2):\n",
    "    return np.dot(v1, v2)\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    return dot_product(v1, v2) / (vector_length(v1) * vector_length(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1 = [1, 2, 3] length = 3.7416573867739413 \n",
      "v2 = [4, 5, 6] length = 3.7416573867739413\n",
      "----------------------------------------------------------------------------------------------------\n",
      "dot_product = 32 \n",
      "cosine_similarity = 0.9746318461970762\n"
     ]
    }
   ],
   "source": [
    "v1 = [1,2,3]\n",
    "v2 = [4,5,6]\n",
    "\n",
    "dp = dot_product(v1, v2)\n",
    "cs = cosine_similarity(v1, v2)\n",
    "\n",
    "print(\"v1 =\", v1,\"length =\", vector_length(v1), \"\\nv2 =\", v2, \"length =\", vector_length(v1))\n",
    "print_seperator()\n",
    "print(\"dot_product =\", dp, \"\\ncosine_similarity =\", cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2dc30ea8811913365942afc3e2c3878a21474b4ff08caf61bb4aba99580c79fd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
